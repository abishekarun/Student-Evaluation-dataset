
<!DOCTYPE html
  PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html><head>
      <meta http-equiv="Content-Type" content="text/html; charset=utf-8">
   <!--
This HTML was auto-generated from MATLAB code.
To make changes, update the MATLAB code and republish this document.
      --><title>Turkiye student Evaluation Dataset</title><meta name="generator" content="MATLAB 9.3"><link rel="schema.DC" href="http://purl.org/dc/elements/1.1/"><meta name="DC.date" content="2018-07-06"><meta name="DC.source" content="student_evaluation.m"><style type="text/css">
html,body,div,span,applet,object,iframe,h1,h2,h3,h4,h5,h6,p,blockquote,pre,a,abbr,acronym,address,big,cite,code,del,dfn,em,font,img,ins,kbd,q,s,samp,small,strike,strong,sub,sup,tt,var,b,u,i,center,dl,dt,dd,ol,ul,li,fieldset,form,label,legend,table,caption,tbody,tfoot,thead,tr,th,td{margin:0;padding:0;border:0;outline:0;font-size:100%;vertical-align:baseline;background:transparent}body{line-height:1}ol,ul{list-style:none}blockquote,q{quotes:none}blockquote:before,blockquote:after,q:before,q:after{content:'';content:none}:focus{outine:0}ins{text-decoration:none}del{text-decoration:line-through}table{border-collapse:collapse;border-spacing:0}

html { min-height:100%; margin-bottom:1px; }
html body { height:100%; margin:0px; font-family:Arial, Helvetica, sans-serif; font-size:10px; color:#000; line-height:140%; background:#fff none; overflow-y:scroll; }
html body td { vertical-align:top; text-align:left; }

h1 { padding:0px; margin:0px 0px 25px; font-family:Arial, Helvetica, sans-serif; font-size:1.5em; color:#d55000; line-height:100%; font-weight:normal; }
h2 { padding:0px; margin:0px 0px 8px; font-family:Arial, Helvetica, sans-serif; font-size:1.2em; color:#000; font-weight:bold; line-height:140%; border-bottom:1px solid #d6d4d4; display:block; }
h3 { padding:0px; margin:0px 0px 5px; font-family:Arial, Helvetica, sans-serif; font-size:1.1em; color:#000; font-weight:bold; line-height:140%; }

a { color:#005fce; text-decoration:none; }
a:hover { color:#005fce; text-decoration:underline; }
a:visited { color:#004aa0; text-decoration:none; }

p { padding:0px; margin:0px 0px 20px; }
img { padding:0px; margin:0px 0px 20px; border:none; }
p img, pre img, tt img, li img, h1 img, h2 img { margin-bottom:0px; } 

ul { padding:0px; margin:0px 0px 20px 23px; list-style:square; }
ul li { padding:0px; margin:0px 0px 7px 0px; }
ul li ul { padding:5px 0px 0px; margin:0px 0px 7px 23px; }
ul li ol li { list-style:decimal; }
ol { padding:0px; margin:0px 0px 20px 0px; list-style:decimal; }
ol li { padding:0px; margin:0px 0px 7px 23px; list-style-type:decimal; }
ol li ol { padding:5px 0px 0px; margin:0px 0px 7px 0px; }
ol li ol li { list-style-type:lower-alpha; }
ol li ul { padding-top:7px; }
ol li ul li { list-style:square; }

.content { font-size:1.2em; line-height:140%; padding: 20px; }

pre, code { font-size:12px; }
tt { font-size: 1.2em; }
pre { margin:0px 0px 20px; }
pre.codeinput { padding:10px; border:1px solid #d3d3d3; background:#f7f7f7; }
pre.codeoutput { padding:10px 11px; margin:0px 0px 20px; color:#4c4c4c; }
pre.error { color:red; }

@media print { pre.codeinput, pre.codeoutput { word-wrap:break-word; width:100%; } }

span.keyword { color:#0000FF }
span.comment { color:#228B22 }
span.string { color:#A020F0 }
span.untermstring { color:#B20000 }
span.syscmd { color:#B28C00 }

.footer { width:auto; padding:10px 0px; margin:25px 0px 0px; border-top:1px dotted #878787; font-size:0.8em; line-height:140%; font-style:italic; color:#878787; text-align:left; float:none; }
.footer p { margin:0px; }
.footer a { color:#878787; }
.footer a:hover { color:#878787; text-decoration:underline; }
.footer a:visited { color:#878787; }

table th { padding:7px 5px; text-align:left; vertical-align:middle; border: 1px solid #d6d4d4; font-weight:bold; }
table td { padding:7px 5px; text-align:left; vertical-align:top; border:1px solid #d6d4d4; }





  </style></head><body><div class="content"><h1>Turkiye student Evaluation Dataset</h1><!--introduction--><p>Unsupervised learning problem</p><!--/introduction--><h2>Contents</h2><div><ul><li><a href="#1">Data Preprocessing</a></li><li><a href="#3">Exploratory Data Analysis</a></li><li><a href="#4">Models with all data</a></li><li><a href="#11">Variables Distribution</a></li><li><a href="#12">Models after removing question columns</a></li></ul></div><h2 id="1">Data Preprocessing</h2><pre class="codeinput">X = readtable(<span class="string">'turkiye-student-evaluation_generic.csv'</span>);

<span class="comment">% Check Missing values</span>
sum(ismissing(X))

<span class="comment">% there are no missing values in any column of this dataset.</span>
<span class="comment">% No need of data preprocessing</span>

<span class="comment">% All Attributes are numeric</span>

<span class="comment">% The nb.repeat is the target column of this dataset. It takes 3 values</span>
<span class="comment">% 1,2,and 3.</span>
</pre><pre class="codeoutput">
ans =

  Columns 1 through 13

     0     0     0     0     0     0     0     0     0     0     0     0     0

  Columns 14 through 26

     0     0     0     0     0     0     0     0     0     0     0     0     0

  Columns 27 through 33

     0     0     0     0     0     0     0

</pre><pre class="codeinput"><span class="comment">% Target variable distribution</span>
tabulate(X.nb_repeat)

<span class="comment">% Split train and target data</span>
Y=X.nb_repeat;
X.nb_repeat=[];
</pre><pre class="codeoutput">  Value    Count   Percent
      1     4909     84.35%
      2      576      9.90%
      3      335      5.76%
</pre><h2 id="3">Exploratory Data Analysis</h2><pre class="codeinput"><span class="keyword">for</span> k=1:10
    colData = X.(k);
    figure(k)
    histogram(colData);
    xlabel(X.Properties.VariableNames(k));
    ylabel(<span class="string">'Count'</span>);
<span class="keyword">end</span>

figure(11)
histogram(Y);
xlabel(<span class="string">'nbRepeat'</span>);
ylabel(<span class="string">'Count'</span>);
</pre><img vspace="5" hspace="5" src="student_evaluation_01.png" alt=""> <img vspace="5" hspace="5" src="student_evaluation_02.png" alt=""> <img vspace="5" hspace="5" src="student_evaluation_03.png" alt=""> <img vspace="5" hspace="5" src="student_evaluation_04.png" alt=""> <img vspace="5" hspace="5" src="student_evaluation_05.png" alt=""> <img vspace="5" hspace="5" src="student_evaluation_06.png" alt=""> <img vspace="5" hspace="5" src="student_evaluation_07.png" alt=""> <img vspace="5" hspace="5" src="student_evaluation_08.png" alt=""> <img vspace="5" hspace="5" src="student_evaluation_09.png" alt=""> <img vspace="5" hspace="5" src="student_evaluation_10.png" alt=""> <img vspace="5" hspace="5" src="student_evaluation_11.png" alt=""> <h2 id="4">Models with all data</h2><pre class="codeinput">rng(<span class="string">'default'</span>) <span class="comment">% For reproducibility</span>

<span class="comment">% Models</span>

<span class="comment">% Naive Bayes</span>
<span class="comment">% K-nearest Neighbor</span>
<span class="comment">% Logistic Regression</span>
<span class="comment">% Decision Trees</span>
<span class="comment">% Random Forest</span>
<span class="comment">% Multi-Layer Perceptron</span>
</pre><p><h3>Naive Bayes</h3></p><pre class="codeinput">Md1 = fitcnb(X,Y);
L1 = loss(Md1,X,Y);
AccuracyNB = 1-L1
</pre><pre class="codeoutput">
AccuracyNB =

    0.6732

</pre><p><h3>KNN</h3></p><pre class="codeinput">Md2 = fitcknn(X,Y,<span class="string">'NumNeighbors'</span>,4);
L2 = loss(Md2,X,Y);
AccuracyKNN = 1-L2
</pre><pre class="codeoutput">
AccuracyKNN =

    0.8625

</pre><p><h3>Logistic Regression</h3></p><pre class="codeinput">[A,~] = mnrfit(table2array(X),Y);
yhat = mnrval(A,table2array(X));
[~,B]=max(yhat');
CP = classperf(Y,B')
</pre><pre class="codeoutput">                        Label: ''
                  Description: ''
                  ClassLabels: [3&times;1 double]
                  GroundTruth: [5820&times;1 double]
         NumberOfObservations: 5820
               ControlClasses: [2&times;1 double]
                TargetClasses: 1
            ValidationCounter: 1
           SampleDistribution: [5820&times;1 double]
            ErrorDistribution: [5820&times;1 double]
    SampleDistributionByClass: [3&times;1 double]
     ErrorDistributionByClass: [3&times;1 double]
               CountingMatrix: [4&times;3 double]
                  CorrectRate: 0.8433
                    ErrorRate: 0.1567
              LastCorrectRate: 0.8433
                LastErrorRate: 0.1567
             InconclusiveRate: 0
               ClassifiedRate: 1
                  Sensitivity: 0.9990
                  Specificity: 0.0055
      PositivePredictiveValue: 0.8441
      NegativePredictiveValue: 0.5000
           PositiveLikelihood: 1.0045
           NegativeLikelihood: 0.1856
                   Prevalence: 0.8435
              DiagnosticTable: [2&times;2 double]

</pre><p><h3>Random Forest</h3></p><pre class="codeinput">B = TreeBagger(50,X,Y,<span class="string">'OOBVarImp'</span>,<span class="string">'On'</span>)
figure
plot(oobError(B))
xlabel(<span class="string">'Number of Grown Trees'</span>)
ylabel(<span class="string">'Out-of-Bag Classification Error'</span>)

mean(oobError(B))
</pre><pre class="codeoutput">
B = 

  TreeBagger
Ensemble with 50 bagged decision trees:
                    Training X:            [5820x32]
                    Training Y:             [5820x1]
                        Method:       classification
                 NumPredictors:                   32
         NumPredictorsToSample:                    6
                   MinLeafSize:                    1
                 InBagFraction:                    1
         SampleWithReplacement:                    1
          ComputeOOBPrediction:                    1
 ComputeOOBPredictorImportance:                    1
                     Proximity:                   []
                    ClassNames:             '1'             '2'             '3'


ans =

    0.1644

</pre><img vspace="5" hspace="5" src="student_evaluation_12.png" alt=""> <p><h3>Decision Tree</h3></p><pre class="codeinput">Md3 = fitctree(X,Y,<span class="string">'MaxNumSplits'</span>,7,<span class="string">'CrossVal'</span>,<span class="string">'on'</span>);
view(Md3.Trained{1},<span class="string">'Mode'</span>,<span class="string">'graph'</span>)
numBranches = @(x)sum(x.IsBranch);
mdlDefaultNumSplits = cellfun(numBranches, Md3.Trained);

figure;
histogram(mdlDefaultNumSplits)

Error3 = kfoldLoss(Md3);
AccuracyDT=1-Error3
</pre><pre class="codeoutput">
AccuracyDT =

    0.8435

</pre><img vspace="5" hspace="5" src="student_evaluation_13.png" alt=""> <img vspace="5" hspace="5" src="student_evaluation_14.png" alt=""> <p><h3>Neural Network</h3></p><pre class="codeinput">x = table2array(X)';
t = full(ind2vec(Y'));

trainFcn = <span class="string">'trainscg'</span>;  <span class="comment">% Scaled conjugate gradient backpropagation.</span>
hiddenLayerSize = 10;
net = patternnet(hiddenLayerSize, trainFcn);
net.input.processFcns = {<span class="string">'removeconstantrows'</span>,<span class="string">'mapminmax'</span>};

net.divideFcn = <span class="string">'dividerand'</span>;  <span class="comment">% Divide data randomly</span>
net.divideMode = <span class="string">'sample'</span>;  <span class="comment">% Divide up every sample</span>
net.divideParam.trainRatio = 70/100;
net.divideParam.valRatio = 15/100;
net.divideParam.testRatio = 15/100;

net.performFcn = <span class="string">'crossentropy'</span>;  <span class="comment">% Cross-Entropy</span>

net.plotFcns = {<span class="string">'plotperform'</span>,<span class="string">'plottrainstate'</span>,<span class="string">'ploterrhist'</span>, <span class="keyword">...</span>
    <span class="string">'plotconfusion'</span>, <span class="string">'plotroc'</span>};

<span class="comment">% Train the Network</span>
[net,tr] = train(net,x,t);

<span class="comment">% Test the Network</span>
y = net(x);
e = gsubtract(t,y);
performance = perform(net,t,y)

<span class="comment">% View the Network</span>
<span class="comment">% view(net)</span>

<span class="comment">% Plots</span>
figure, plotperform(tr)
figure, plottrainstate(tr)
figure, ploterrhist(e)
figure, plotconfusion(t,y)
figure, plotroc(t,y)
</pre><pre class="codeoutput">
performance =

    0.1685

</pre><img vspace="5" hspace="5" src="student_evaluation_15.png" alt=""> <img vspace="5" hspace="5" src="student_evaluation_16.png" alt=""> <img vspace="5" hspace="5" src="student_evaluation_17.png" alt=""> <img vspace="5" hspace="5" src="student_evaluation_18.png" alt=""> <img vspace="5" hspace="5" src="student_evaluation_19.png" alt=""> <h2 id="11">Variables Distribution</h2><pre class="codeinput">variables=X.Properties.VariableNames; <span class="comment">%get variable names</span>
tot=zeros(5,1);
<span class="keyword">for</span> k=6:length(variables)
    colData = X.(k);
    temp=tabulate(colData);
    tot = tot + temp(:,3);
<span class="keyword">end</span>
tot=tot/(length(variables)-6)

<span class="comment">% Looks like all variables have similar distribution</span>
</pre><pre class="codeoutput">
tot =

   15.6483
   12.1722
   29.8639
   28.5375
   17.6242

</pre><h2 id="12">Models after removing question columns</h2><p>Lets remove unnecessary question columns as they have same distribution</p><pre class="codeinput"><span class="comment">% Models</span>

<span class="comment">% Naive Bayes</span>
<span class="comment">% K-nearest Neighbor</span>
<span class="comment">% Logistic Regression</span>
<span class="comment">% Decision Trees</span>
<span class="comment">% Random Forest</span>
<span class="comment">% Multi-Layer Perceptron</span>
</pre><p><h3>Naive Bayes</h3></p><pre class="codeinput">mod_dat = X(:,1:4);
Md4 = fitcnb(mod_dat,Y);
L4 = loss(Md4,mod_dat,Y);
AccuracyNB=1-L4
</pre><pre class="codeoutput">
AccuracyNB =

    0.8435

</pre><p><h3>KNN</h3></p><pre class="codeinput">mod_dat = X(:,1:4);
Md5 = fitcknn(mod_dat,Y,<span class="string">'NumNeighbors'</span>,4);
L5 = loss(Md5,mod_dat,Y);
AccuracyKNN=1-L5
</pre><pre class="codeoutput">
AccuracyKNN =

    0.8206

</pre><p><h3>Logistic Regression</h3></p><pre class="codeinput">[A,dev,stats] = mnrfit(table2array(mod_dat),Y);
yhat = mnrval(A,table2array(mod_dat));
[~,B]=max(yhat');
CP = classperf(Y,B')
</pre><pre class="codeoutput">                        Label: ''
                  Description: ''
                  ClassLabels: [3&times;1 double]
                  GroundTruth: [5820&times;1 double]
         NumberOfObservations: 5820
               ControlClasses: [2&times;1 double]
                TargetClasses: 1
            ValidationCounter: 1
           SampleDistribution: [5820&times;1 double]
            ErrorDistribution: [5820&times;1 double]
    SampleDistributionByClass: [3&times;1 double]
     ErrorDistributionByClass: [3&times;1 double]
               CountingMatrix: [4&times;3 double]
                  CorrectRate: 0.8435
                    ErrorRate: 0.1565
              LastCorrectRate: 0.8435
                LastErrorRate: 0.1565
             InconclusiveRate: 0
               ClassifiedRate: 1
                  Sensitivity: 1
                  Specificity: 0
      PositivePredictiveValue: 0.8435
      NegativePredictiveValue: NaN
           PositiveLikelihood: 1
           NegativeLikelihood: NaN
                   Prevalence: 0.8435
              DiagnosticTable: [2&times;2 double]

</pre><p><h3>Decision Tree</h3></p><pre class="codeinput">Md6 = fitctree(mod_dat,Y,<span class="string">'MaxNumSplits'</span>,7,<span class="string">'CrossVal'</span>,<span class="string">'on'</span>);
view(Md6.Trained{1},<span class="string">'Mode'</span>,<span class="string">'graph'</span>);
numBranches = @(x)sum(x.IsBranch);
mdlDefaultNumSplits = cellfun(numBranches, Md6.Trained);

figure;
histogram(mdlDefaultNumSplits);

Error6 = kfoldLoss(Md6);
AccuracyDT=1-Error6
</pre><pre class="codeoutput">
AccuracyDT =

    0.8435

</pre><img vspace="5" hspace="5" src="student_evaluation_20.png" alt=""> <img vspace="5" hspace="5" src="student_evaluation_21.png" alt=""> <p><h3>Random Forest</h3></p><pre class="codeinput">B = TreeBagger(50,mod_dat,Y,<span class="string">'OOBVarImp'</span>,<span class="string">'On'</span>);
figure
plot(oobError(B))
xlabel(<span class="string">'Number of Grown Trees'</span>);
ylabel(<span class="string">'Out-of-Bag Classification Error'</span>);

mean(oobError(B))
</pre><pre class="codeoutput">
ans =

    0.1581

</pre><img vspace="5" hspace="5" src="student_evaluation_22.png" alt=""> <p><h3>Neural Network</h3></p><pre class="codeinput">x = table2array(mod_dat)';
t = full(ind2vec(Y'));

trainFcn = <span class="string">'trainscg'</span>;  <span class="comment">% Scaled conjugate gradient backpropagation.</span>
hiddenLayerSize = 10;
net = patternnet(hiddenLayerSize, trainFcn);
net.input.processFcns = {<span class="string">'removeconstantrows'</span>,<span class="string">'mapminmax'</span>};

net.divideFcn = <span class="string">'dividerand'</span>;  <span class="comment">% Divide data randomly</span>
net.divideMode = <span class="string">'sample'</span>;  <span class="comment">% Divide up every sample</span>
net.divideParam.trainRatio = 70/100;
net.divideParam.valRatio = 15/100;
net.divideParam.testRatio = 15/100;

net.performFcn = <span class="string">'crossentropy'</span>;  <span class="comment">% Cross-Entropy</span>

net.plotFcns = {<span class="string">'plotperform'</span>,<span class="string">'plottrainstate'</span>,<span class="string">'ploterrhist'</span>, <span class="keyword">...</span>
    <span class="string">'plotconfusion'</span>, <span class="string">'plotroc'</span>};

<span class="comment">% Train the Network</span>
[net,tr] = train(net,x,t);

<span class="comment">% Test the Network</span>
y = net(x);
e = gsubtract(t,y);
performance = perform(net,t,y)

<span class="comment">% View the Network</span>
<span class="comment">% view(net)</span>

<span class="comment">% Plots</span>
figure, plotperform(tr)
figure, plottrainstate(tr)
figure, ploterrhist(e)
figure, plotconfusion(t,y)
figure, plotroc(t,y)
</pre><pre class="codeoutput">
performance =

    0.1595

</pre><img vspace="5" hspace="5" src="student_evaluation_23.png" alt=""> <img vspace="5" hspace="5" src="student_evaluation_24.png" alt=""> <img vspace="5" hspace="5" src="student_evaluation_25.png" alt=""> <img vspace="5" hspace="5" src="student_evaluation_26.png" alt=""> <img vspace="5" hspace="5" src="student_evaluation_27.png" alt=""> <p class="footer"><br><a href="http://www.mathworks.com/products/matlab/">Published with MATLAB&reg; R2017b</a><br></p></div><!--
##### SOURCE BEGIN #####
%% Turkiye student Evaluation Dataset
% Unsupervised learning problem

%% Data Preprocessing
X = readtable('turkiye-student-evaluation_generic.csv');

% Check Missing values
sum(ismissing(X))

% there are no missing values in any column of this dataset.
% No need of data preprocessing

% All Attributes are numeric

% The nb.repeat is the target column of this dataset. It takes 3 values
% 1,2,and 3.

%%

% Target variable distribution
tabulate(X.nb_repeat)

% Split train and target data
Y=X.nb_repeat;
X.nb_repeat=[];
%% Exploratory Data Analysis 

for k=1:10
    colData = X.(k);
    figure(k)
    histogram(colData);
    xlabel(X.Properties.VariableNames(k));
    ylabel('Count');
end

figure(11)
histogram(Y);
xlabel('nbRepeat');
ylabel('Count');
%% Models with all data
rng('default') % For reproducibility

% Models 

% Naive Bayes
% K-nearest Neighbor
% Logistic Regression
% Decision Trees
% Random Forest
% Multi-Layer Perceptron

%% 
% <html><h3>Naive Bayes</h3></html>

Md1 = fitcnb(X,Y);
L1 = loss(Md1,X,Y);
AccuracyNB = 1-L1
%% 
% <html><h3>KNN</h3></html>
Md2 = fitcknn(X,Y,'NumNeighbors',4);
L2 = loss(Md2,X,Y);
AccuracyKNN = 1-L2
%% 
% <html><h3>Logistic Regression</h3></html>

[A,~] = mnrfit(table2array(X),Y);
yhat = mnrval(A,table2array(X));
[~,B]=max(yhat');
CP = classperf(Y,B')

%% 
% <html><h3>Random Forest</h3></html>

B = TreeBagger(50,X,Y,'OOBVarImp','On')
figure
plot(oobError(B))
xlabel('Number of Grown Trees')
ylabel('Out-of-Bag Classification Error')

mean(oobError(B))
%% 
% <html><h3>Decision Tree</h3></html>
Md3 = fitctree(X,Y,'MaxNumSplits',7,'CrossVal','on');
view(Md3.Trained{1},'Mode','graph')
numBranches = @(x)sum(x.IsBranch);
mdlDefaultNumSplits = cellfun(numBranches, Md3.Trained);

figure;
histogram(mdlDefaultNumSplits)

Error3 = kfoldLoss(Md3);
AccuracyDT=1-Error3
%% 
% <html><h3>Neural Network</h3></html>

x = table2array(X)';
t = full(ind2vec(Y'));

trainFcn = 'trainscg';  % Scaled conjugate gradient backpropagation.
hiddenLayerSize = 10;
net = patternnet(hiddenLayerSize, trainFcn);
net.input.processFcns = {'removeconstantrows','mapminmax'};

net.divideFcn = 'dividerand';  % Divide data randomly
net.divideMode = 'sample';  % Divide up every sample
net.divideParam.trainRatio = 70/100;
net.divideParam.valRatio = 15/100;
net.divideParam.testRatio = 15/100;

net.performFcn = 'crossentropy';  % Cross-Entropy

net.plotFcns = {'plotperform','plottrainstate','ploterrhist', ...
    'plotconfusion', 'plotroc'};

% Train the Network
[net,tr] = train(net,x,t);

% Test the Network
y = net(x);
e = gsubtract(t,y);
performance = perform(net,t,y)

% View the Network
% view(net)

% Plots
figure, plotperform(tr)
figure, plottrainstate(tr)
figure, ploterrhist(e)
figure, plotconfusion(t,y)
figure, plotroc(t,y)

%% Variables Distribution
variables=X.Properties.VariableNames; %get variable names
tot=zeros(5,1);
for k=6:length(variables)
    colData = X.(k);
    temp=tabulate(colData);
    tot = tot + temp(:,3);
end
tot=tot/(length(variables)-6)

% Looks like all variables have similar distribution

%% Models after removing question columns
% Lets remove unnecessary question columns as they have same distribution

% Models 

% Naive Bayes
% K-nearest Neighbor
% Logistic Regression
% Decision Trees
% Random Forest
% Multi-Layer Perceptron
%%
% <html><h3>Naive Bayes</h3></html>
mod_dat = X(:,1:4);
Md4 = fitcnb(mod_dat,Y);
L4 = loss(Md4,mod_dat,Y);
AccuracyNB=1-L4
%% 
% <html><h3>KNN</h3></html>
mod_dat = X(:,1:4);
Md5 = fitcknn(mod_dat,Y,'NumNeighbors',4);
L5 = loss(Md5,mod_dat,Y);
AccuracyKNN=1-L5
%% 
% <html><h3>Logistic Regression</h3></html>
[A,dev,stats] = mnrfit(table2array(mod_dat),Y);
yhat = mnrval(A,table2array(mod_dat));
[~,B]=max(yhat');
CP = classperf(Y,B')

%% 
% <html><h3>Decision Tree</h3></html>
Md6 = fitctree(mod_dat,Y,'MaxNumSplits',7,'CrossVal','on');
view(Md6.Trained{1},'Mode','graph');
numBranches = @(x)sum(x.IsBranch);
mdlDefaultNumSplits = cellfun(numBranches, Md6.Trained);

figure;
histogram(mdlDefaultNumSplits);

Error6 = kfoldLoss(Md6);
AccuracyDT=1-Error6
%% 
% <html><h3>Random Forest</h3></html>

B = TreeBagger(50,mod_dat,Y,'OOBVarImp','On');
figure
plot(oobError(B))
xlabel('Number of Grown Trees');
ylabel('Out-of-Bag Classification Error');

mean(oobError(B))

%% 
% <html><h3>Neural Network</h3></html>

x = table2array(mod_dat)';
t = full(ind2vec(Y'));

trainFcn = 'trainscg';  % Scaled conjugate gradient backpropagation.
hiddenLayerSize = 10;
net = patternnet(hiddenLayerSize, trainFcn);
net.input.processFcns = {'removeconstantrows','mapminmax'};

net.divideFcn = 'dividerand';  % Divide data randomly
net.divideMode = 'sample';  % Divide up every sample
net.divideParam.trainRatio = 70/100;
net.divideParam.valRatio = 15/100;
net.divideParam.testRatio = 15/100;

net.performFcn = 'crossentropy';  % Cross-Entropy

net.plotFcns = {'plotperform','plottrainstate','ploterrhist', ...
    'plotconfusion', 'plotroc'};

% Train the Network
[net,tr] = train(net,x,t);

% Test the Network
y = net(x);
e = gsubtract(t,y);
performance = perform(net,t,y)

% View the Network
% view(net)

% Plots
figure, plotperform(tr)
figure, plottrainstate(tr)
figure, ploterrhist(e)
figure, plotconfusion(t,y)
figure, plotroc(t,y)
##### SOURCE END #####
--></body></html>